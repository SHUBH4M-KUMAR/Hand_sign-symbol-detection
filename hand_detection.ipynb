{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6426383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7ab6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba532a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e741cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "def load_object_detection_model(checkpoint_path, pipeline_config_path):\n",
    "    # Load the pipeline config\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "\n",
    "    # Build the detection model\n",
    "    detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "    # Restore the checkpoint\n",
    "    ckpt = tf.train.Checkpoint(model=detection_model)\n",
    "    ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "    return detection_model\n",
    "\n",
    "# Example usage\n",
    "checkpoint_path = r\"C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\after_5000_steps\\ckpt-2.0002.index\"  # Replace with your actual path\n",
    "pipeline_config_path = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\pipeline.config'  # Replace with your actual path\n",
    "\n",
    "loaded_model = load_object_detection_model(checkpoint_path, pipeline_config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "561f90c6-b44d-4e14-a6a5-a082d016aba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SSDMetaArch' object has no attribute '_build_model_with_keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m output_h5_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mshubh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mML Models\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTensorflow\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhand_sign_detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mafter_5000_steps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your desired output path\u001b[39;00m\n\u001b[0;32m     36\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m load_object_detection_model(checkpoint_path, pipeline_config_path)\n\u001b[1;32m---> 37\u001b[0m \u001b[43msave_model_to_h5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_h5_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m, in \u001b[0;36msave_model_to_h5\u001b[1;34m(model, output_h5_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_model_to_h5\u001b[39m(model, output_h5_path):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Convert the Object Detection API model to a Keras model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     keras_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model_with_keras\u001b[49m()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Create a dummy input to save the model architecture\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     dummy_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, height, width, channels)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SSDMetaArch' object has no attribute '_build_model_with_keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "def load_object_detection_model(checkpoint_path, pipeline_config_path):\n",
    "    # Load the pipeline config\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "\n",
    "    # Build the detection model\n",
    "    detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "    # Restore the checkpoint\n",
    "    ckpt = tf.train.Checkpoint(model=detection_model)\n",
    "    ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "    return detection_model\n",
    "\n",
    "def save_model_to_h5(model, output_h5_path):\n",
    "    # Convert the Object Detection API model to a Keras model\n",
    "    keras_model = model._build_model_with_keras()\n",
    "\n",
    "    # Create a dummy input to save the model architecture\n",
    "    dummy_input = tf.constant(np.random.rand(1, height, width, channels).astype(np.float32))\n",
    "\n",
    "    # Perform a forward pass to create the model architecture\n",
    "    _ = keras_model(dummy_input)\n",
    "\n",
    "    # Save the entire model to the HDF5 file using Keras save_model function\n",
    "    tf.keras.models.save_model(keras_model, output_h5_path, include_optimizer=False)\n",
    "\n",
    "# Example usage\n",
    "checkpoint_path = r\"C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\after_5000_steps\\ckpt-2.0002.index\"  # Replace with your actual path\n",
    "pipeline_config_path = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\pipeline.config'  # Replace with your actual path\n",
    "output_h5_path = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\after_5000_steps\\model.h5'  # Replace with your desired output path\n",
    "\n",
    "loaded_model = load_object_detection_model(checkpoint_path, pipeline_config_path)\n",
    "save_model_to_h5(loaded_model, output_h5_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0564d1d6-6455-4d19-aa39-411eb0f5ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def check_model_loaded(model):\n",
    "    # Create a sample input (you may need to adjust this based on your model's input requirements)\n",
    "    sample_input = np.random.rand(1, height, width, channels).astype(np.float32)\n",
    "\n",
    "    # Perform inference\n",
    "    detections = model(sample_input)\n",
    "\n",
    "    # Check if the output is non-empty (indicating a successful inference)\n",
    "    if detections['num_detections'][0] > 0:\n",
    "        print(\"Model loaded successfully.\")\n",
    "    else:\n",
    "        print(\"There might be an issue with the loaded model.\")\n",
    "\n",
    "# Replace with your actual input dimensions (height, width, channels)\n",
    "height, width, channels = 256, 256, 3\n",
    "\n",
    "# Example usage\n",
    "check_model_loaded(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bc576c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SSDMetaArch' object has no attribute 'save_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m prediction_dict \u001b[38;5;241m=\u001b[39m detection_model\u001b[38;5;241m.\u001b[39mpredict(image, shapes)\n\u001b[0;32m     18\u001b[0m detections \u001b[38;5;241m=\u001b[39m detection_model\u001b[38;5;241m.\u001b[39mpostprocess(prediction_dict, shapes)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mdetection_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetection_model_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create a new model and load weights\u001b[39;00m\n\u001b[0;32m     22\u001b[0m new_detection_model \u001b[38;5;241m=\u001b[39m model_builder\u001b[38;5;241m.\u001b[39mbuild(model_config\u001b[38;5;241m=\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SSDMetaArch' object has no attribute 'save_weights'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "CONFIG_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\pipeline.config'\n",
    "CHECKPOINT_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\output'\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b020d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\Train\\hand-sign_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3ca564",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11576\\1351747054.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Setup capture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_FRAME_WIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_FRAME_HEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddef6448",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: (1,)\n",
      "Input Tensor Content: [nan]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Input Tensor Content:\", input_tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17790a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor((image_np_expanded), dtype=tf.float32)\n",
    "\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.6,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb9d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e85efed3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9076\\3726336699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Check the shape of the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mcontinue\u001b[0m  \u001b[1;31m# Skip frames that are not 3D images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70630093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0922bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from streamlit_webrtc import VideoTransformerBase, webrtc_streamer\n",
    "import os\n",
    "\n",
    "# Add the path to your pipeline.config and checkpoint folder\n",
    "CONFIG_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\pipeline.config'\n",
    "CHECKPOINT_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\output'\n",
    "\n",
    "# Add the path to your label map file\n",
    "LABEL_MAP_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\Train\\hand-sign_label_map.pbtxt'\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "try:\n",
    "    ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading the model:\", e)\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
    "\n",
    "class ObjectDetectionTransformer(VideoTransformerBase):\n",
    "    def __init__(self):\n",
    "        self.detection_model = detection_model\n",
    "        self.category_index = category_index\n",
    "\n",
    "    def transform(self, frame):\n",
    "        image_np = np.array(frame)\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "        input_tensor = tf.convert_to_tensor(image_np_expanded, dtype=tf.float32)\n",
    "\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        detections['num_detections'] = num_detections\n",
    "\n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = image_np.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes'] + label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            self.category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5,\n",
    "            min_score_thresh=0.6,\n",
    "            agnostic_mode=False\n",
    "        )\n",
    "        print(\"Detection Scores:\", detections['detection_scores'])\n",
    "        print(\"Detection Classes:\", detections['detection_classes'])\n",
    "\n",
    "        return cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def main():\n",
    "    st.title(\"Object Detection with Streamlit and Webcam\")\n",
    "\n",
    "    webrtc_ctx = webrtc_streamer(\n",
    "        key=\"example\",\n",
    "        video_transformer_factory=ObjectDetectionTransformer,\n",
    "        async_transform=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from streamlit_webrtc import VideoTransformerBase, webrtc_streamer\n",
    "import os\n",
    "\n",
    "# Add the path to your pipeline.config and checkpoint folder\n",
    "CONFIG_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\pipeline.config'\n",
    "CHECKPOINT_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\output'\n",
    "\n",
    "# Add the path to your label map file\n",
    "LABEL_MAP_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\Train\\hand-sign_label_map.pbtxt'\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "try:\n",
    "    ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading the model:\", e)\n",
    "    st.error(\"Error loading the model. Check the console for details.\")\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
    "\n",
    "\n",
    "input_type = st.sidebar.selectbox(\"Select Input Type\", [\"Select\", \"Webcam\", \"Image\", \"Video\"], index=0)\n",
    "\n",
    "# Initialize video capture based on user choice\n",
    "if input_type == \"Webcam\":\n",
    "    cap = cv2.VideoCapture(0)\n",
    "elif input_type == \"Image\":\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Choose an image...\", type=\"jpg\")\n",
    "    if uploaded_file is not None:\n",
    "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
    "        img = cv2.imdecode(file_bytes, 1)\n",
    "else:  # Video\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Choose a video...\", type=\"mp4\")\n",
    "    if uploaded_file is not None:\n",
    "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
    "        video = cv2.VideoCapture()\n",
    "        video.open(uploaded_file.name)\n",
    "\n",
    "while True:\n",
    "    if input_type == \"Select\":\n",
    "        st.warning(\"Please select an input type from the sidebar.\")\n",
    "        break\n",
    "    elif input_type == \"Webcam\":\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error reading from webcam.\")\n",
    "            break\n",
    "    elif input_type == \"Image\":\n",
    "        if uploaded_file is not None:\n",
    "            frame = img.copy()\n",
    "        else:\n",
    "            break\n",
    "    else:  # Video\n",
    "        if uploaded_file is not None:\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                print(\"Error reading from video.\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "# Setup capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error reading from webcam.\")\n",
    "        break\n",
    "\n",
    "    image_np = np.array(frame)\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(image_np_expanded, dtype=tf.float32)\n",
    "\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_detections,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + label_id_offset,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b94bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "from streamlit_webrtc import VideoTransformerBase, webrtc_streamer\n",
    "import os\n",
    "\n",
    "# Add the path to your pipeline.config and checkpoint folder\n",
    "CONFIG_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\pipeline.config'\n",
    "CHECKPOINT_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\output'\n",
    "\n",
    "# Add the path to your label map file\n",
    "LABEL_MAP_PATH = r'C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\\Train\\hand-sign_label_map.pbtxt'\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "try:\n",
    "    ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading the model:\", e)\n",
    "    st.error(\"Error loading the model. Check the console for details.\")\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections\n",
    "\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH)\n",
    "\n",
    "\n",
    "def detect_objects(image_np):\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    input_tensor = tf.convert_to_tensor(image_np_expanded, dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_detections,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + label_id_offset,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.6,\n",
    "        agnostic_mode=False)\n",
    "\n",
    "    # Convert color space from BGR to RGB\n",
    "    image_np_with_detections = cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return image_np_with_detections\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.title(\"Object Detection with Streamlit\")\n",
    "    input_types = [\"Select\", \"Webcam\", \"Image\", \"Video\"]\n",
    "    input_type = st.sidebar.selectbox(\"Select Input Type\", input_types, index=0)\n",
    "\n",
    "    if input_type == \"Select\":\n",
    "        st.warning(\"Please select an input type from the sidebar.\")\n",
    "    elif input_type == \"Webcam\":\n",
    "        st.warning(\"To stop the webcam, press 'q' key.\")\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                st.error(\"Error reading from webcam.\")\n",
    "                break\n",
    "            image = detect_objects(frame)\n",
    "            st.image(image, channels=\"RGB\")\n",
    "    elif input_type == \"Image\":\n",
    "        uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "        if uploaded_file is not None:\n",
    "            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
    "            img = cv2.imdecode(file_bytes, 1)\n",
    "            image = detect_objects(img)\n",
    "            st.image(image, channels=\"RGB\", use_column_width=True)\n",
    "    elif input_type == \"Video\":\n",
    "        uploaded_file = st.file_uploader(\"Choose a video...\", type=[\"mp4\"])\n",
    "        if uploaded_file is not None:\n",
    "            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
    "            video = cv2.VideoCapture()\n",
    "            video.open(uploaded_file.name)\n",
    "            while True:\n",
    "                ret, frame = video.read()\n",
    "                if not ret:\n",
    "                    st.error(\"Error reading from video.\")\n",
    "                    break\n",
    "                image = detect_objects(frame)\n",
    "                st.image(image, channels=\"RGB\")\n",
    "    else:\n",
    "        st.warning(\"Please select a valid input type from the sidebar.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1504a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1bb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70beea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac1819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bd61de7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1456\\2372807603.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mnum_detections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'num_detections'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfod-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tfod-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    954\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m         raise ValueError(\"Creating variables on a non-first call to a function\"\n\u001b[0m\u001b[0;32m    957\u001b[0m                          \" decorated with tf.function.\")\n\u001b[0;32m    958\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c2ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814025c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0068f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af67d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b7c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12919a72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'take_photo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1456\\1789326253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtake_photo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mimage_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'take_photo' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244b2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da074d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae59b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e99f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80534800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247a077c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shubh\\\\OneDrive\\\\Desktop\\\\ML Models\\\\Tensorflow'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35b3f69b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\\hand_sign_detection\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307f2e58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\OneDrive\\Desktop\\ML Models\\Tensorflow\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f7cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
